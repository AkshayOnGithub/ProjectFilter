{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Goal of the notebook\n",
    "\n",
    "## Context\n",
    "Our filter is supposed to detect Out-Of-Domain (OOD) sentences from In-Domain (ID) sentences in a conversation between a support agent and a customer.\n",
    "Here the domain (ID) is the full FAQ scrapped from the Europcar website (from https://faq.europcar.com/ and subpages). The domain is \"Car rental\".\n",
    "\n",
    "## Goal\n",
    "\n",
    "Visualize the (semantic) sentence embeddings (generated by a Hugging Face SentenceTransformer) to see if we can spot some patterns or problems.\n",
    "\n",
    "Visualization is done by generating embeddings in a format suitable to use in the Tensorflow Embedding Projector: https://projector.tensorflow.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First we load un hugging face model (see other python files for details). Embedder model is `all-MiniLM-L6-v2` which is marked as suitable for similarity comparisons with euclidian distances. (Important because we want to fit a Guassian\n",
    "to those)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: GaussianEmbeddingsAnomalyDetector\n",
      "Train params used: \n",
      "{'embedder_name': 'all-MiniLM-L6-v2', 'robust_covariance': True}\n",
      "Files used for training:\n",
      "FilterTrainFiles(train_id=['europcar'], train_ood=[], validation_id='validation_id', validation_ood='validation_ood')\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pipelines.persistence import load_pipeline\n",
    "from dataload.dataloading import DataFilesRegistry\n",
    "from pipelines.impl.anomaly_detection import GaussianEmbeddingsAnomalyDetector\n",
    "\n",
    "CAPSTONE_FOLDER = Path(\"/Users/jlinho/Desktop/capstone/\")\n",
    "DATAFILES_FOLDER = CAPSTONE_FOLDER / \"datasources\"\n",
    "MODELS_FOLDER = CAPSTONE_FOLDER / \"models\"\n",
    "\n",
    "data_registry = DataFilesRegistry(DATAFILES_FOLDER)\n",
    "detector = load_pipeline(MODELS_FOLDER / \"20220127_16-50-10\", GaussianEmbeddingsAnomalyDetector)\n",
    "\n",
    "\n",
    "print(f\"Name: {detector.name}\")\n",
    "print(\"Train params used: \")\n",
    "print(detector.run_params)\n",
    "print(\"Files used for training:\")\n",
    "print(detector.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we generate all embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pipelines.impl.anomaly_detection import _file_sentences\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "id_sentences = list(_file_sentences(\"europcar\", data_registry))\n",
    "\n",
    "writer = SummaryWriter(log_dir=(CAPSTONE_FOLDER / \"tensorboard_logs\"))\n",
    "\n",
    "writer.add_embedding(\n",
    "    np.vstack([detector.embedding(s) for s in id_sentences if s.strip() != \"\"]),\n",
    "    metadata=[s for s in id_sentences if s.strip() != \"\"],\n",
    "    tag=detector.embedder_name,\n",
    ")\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard  --logdir \"/Users/jlinho/Desktop/capstone/tensorboard_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some visualizations with UMAP algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![embeddings1](useless_sentences.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can clearly see 2 clusters:\n",
    "- the first one containing the \"car rental\" domain sentences\n",
    "- another containing politeness sentences like \"Please feel free to tell us if you found answer helpful\"\n",
    "\n",
    "If we want to fit cleanly the ID sentences those need to be eliminated (or their effect mitigated) before fitting a gaussian to the main cloud of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to fit a guassian **like this** : ![good](good.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "But not like this ![not good](not_good.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "source": [
    "But not like this ![not good](/Users/jlinho/MyGit/Capstone/Project/notebooks/not_good.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Also small sentences like \"Yes\", \"No\" are should be eliminated ![yes](yes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Multiple ways to eliminate the noise from the ID sentence embeddings are to be explored:\n",
    "- Using Robust Covariance estimator that allows to include only the most similiar 80% (or 90%) of points\n",
    "- Embeddings paragraphs instead of sentences to dilute the effect of those sentences\n",
    "- Try to isolate them automatically by some algorithm (like we did visually above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
