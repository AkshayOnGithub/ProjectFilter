{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Walk-Through"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on the most difficult of our datasets (FDA) because it contains a lot fo technical terms (Drug names, illnesses, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from dataload.dataloading import DataFilesRegistry\n",
    "\n",
    "dataset_dir = Path(Path.cwd() / \"../datasets\")\n",
    "print(f\"Datasets dir: {dataset_dir} , exists {dataset_dir.exists()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First, we list available datasets. DataFilesRegistry is an object that abstracts away the location of the folder and simplifies loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "datasets = DataFilesRegistry(dataset_dir)\n",
    "\n",
    "for idx, item in enumerate(datasets.keys()):\n",
    "    # add new line each 5 iterations\n",
    "    if idx % 5 == 0:\n",
    "        print()\n",
    "    print(item, end=\", \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load just one dataset: fda. A dataset is organized into one or multiple paragraphs, where each paragraph can be multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "paragraphs = datasets.load_items(\"fda\")\n",
    "print(f\"Loaded {len(paragraphs)} paragraphs\")\n",
    "\n",
    "se_paragraphs = pd.Series(paragraphs)\n",
    "pd.DataFrame(se_paragraphs, columns=[\"paragraph\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the somes named entities in our text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Finding important Named Entities\n",
    "\n",
    "In this section, we will attempt to find important terms. This is important because we want latter use \"Huggingface Transformer\" model for sentence embeddings, and each pre-trained transformer has a vocabulary: we want to make sure those important accronyms, or \"Proper Nouns\" are part of the core vocabulary items meaning the HFTransformer has probably a good sense of the meaning of the word..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Finding important words using detection of \"Named Entities\"\n",
    "First we find named entity using NLTK. It uses grammar rules, as well as casing and punctuation to find out the important \"Named Entities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from analysis import words\n",
    "\n",
    "words.find_named_enties(se_paragraphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Finding important words using word frequencies\n",
    "\n",
    "`ParagraphTransform` is a tool developped by our team that chains a series of text transformations(Cleanups) on paragraphs. Let's run some cleanups before counting words. \n",
    "\n",
    "Note that this `ParagraphTransform` will be used later as part of a text pre-processing step, in particular because our AI algorithm requires the proper demarcation of sentences and multiple things (bad punctuation, blanks, misplaced upper-case letters, abbreviations) can prevent a proper demarcation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "words.find_most_frequent_words(se_paragraphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.3 Putting things together\n",
    "\n",
    "As both previous steps have a fair bit of imprecision, we consider the \"top entites\" as those that match both criterias: being detected as Named Entity by NLTK and being a frequent word.\n",
    "\n",
    "Let's join together the top words and the named entities on the lower case version of the word..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_important_words = words.find_top_named_entities(se_paragraphs)\n",
    "df_important_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Sentence splitting\n",
    "\n",
    "As our further model works on sentences, it is important that we are able to split the paragraphs in text into proper sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 Prepare data for proper sentence splitting\n",
    "\n",
    "We again use `ParagraphTransform` to cleanup the data but this time in order to get allow NLTK to proper split the sentences: NLTK tipycally expects a dot to be followed by a space at end of sentences and followed by a upper-case letter. So it gets fooled by abbreviations or accronyms (That contain dots), sentences that do not start by a space after previous sentence's dot. Also, we add cleanup to normalize URLs and Emails in text as we do want the model to understand all of those as just \"a link\" or \"an email\"... Here are some cleanup available:\n",
    "\n",
    "- `spaces` replaces series of spaces and tabs by a single space\"\n",
    "- `sentences_starts` Ensures a sentence end is followed by a space and a Capital letter\"\n",
    "- `uri` Replaces URLs by the placeholder WEBLINK\"\n",
    "- `email` Removes all emails by the placeholder WEBMAIL\"\n",
    "- `common_abbr` Expands common abbreviations (like i.e. into for instance) so that no dot remains\n",
    "- `hyphens` Replaces hyphens in hyphenated words by spaces\n",
    "- `no_stop_words` Removes stop words: it is not clear if this could bring value before using Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# All available paragraph normalizers are in coded in our codebase (mostly using NLTK and regex )\n",
    "from pipelines.impl.preprocessing import get_available_normalizers\n",
    "get_available_normalizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pipelines.impl.preprocessing import make_text_normalizer, split_into_sentences, get_available_normalizers, normalize_sent_min_words\n",
    "from pipelines.impl.paragraph import ParagraphTransform\n",
    "\n",
    "activated_cleanups = ['spaces', 'sentences_start', 'uri', 'email', 'common_abbr']\n",
    "preprocessing_pipe = ParagraphTransform([\n",
    "    make_text_normalizer(activated_cleanups),\n",
    "    split_into_sentences,\n",
    "    normalize_sent_min_words\n",
    "], unique_sentences=True)  # only unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_sentences = preprocessing_pipe.transform(se_paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# print Sentences length vs df_raw length\n",
    "print(f\"Unique Sentences count: {len(df_sentences)}\")\n",
    "print(f\"Paragraphs (or sections) count: {len(se_paragraphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_sentences.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Sentence splitting as a step in our model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create an untrained instance of our model (see code in related files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pipelines.impl.anomaly_detection import GaussianEmbeddingsAnomalyDetector\n",
    "from pipelines.filtering import FilterTrainFiles\n",
    "\n",
    "run_params = {\n",
    "    \"embedder_name\": \"all-MiniLM-L6-v2\",\n",
    "    \"robust_covariance\": True,\n",
    "    \"text_normalizer_keys\": activated_cleanups, \n",
    "    \"support_fraction\": 0.90,\n",
    "}\n",
    "model_datasets = FilterTrainFiles(train_id=\"fda\", validation_id=\"validation_fda_id\", validation_ood=\"validation_fda_ood\")\n",
    "\n",
    "model = GaussianEmbeddingsAnomalyDetector(run_params=run_params, datasets=model_datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the `sentence_splitter` steps that does both the text normalizatins and the sentence splitting... We also set the `embedder` variable to point to the Hugging Face Transformer we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_sentences = model.sentence_splitter.transform(se_paragraphs)\n",
    "df_embeddings = model.embedder.fit_transform(df_sentences)\n",
    "embedder = model.embedder.embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Check tokenization\n",
    "\n",
    "In this section, we check that all \"Important named entites\" are part of a single vocabulary token in the \"Hugging Face Transformer\". In such a transformer, the most important tokens have their own entry in the vocabulary (meaning the Transformer understands them well) and other less important words are split into sub-tokens (like Embedding can become 2 tokens `embedd` and `##ing` )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now find tokens that are split into the Hugging Face vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# find out split words\n",
    "from analysis import tokens\n",
    "\n",
    "split_words = tokens.find_vocab_split_words(df_sentences, embedder.tokenizer)     \n",
    "split_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small problem... To have the best results we want the most important/frequent words like `COVID` `COVID-19` to be encoded on a single token in itself... But we have `co` and `##vid` as sub-tokens... Same for `pandemic`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "important_word_list = df_important_words[\"entity_name\"].values\n",
    "split_words_list = split_words[\"word\"].values\n",
    "# Compute intersection\n",
    "df_import_split_words = set(important_word_list).intersection(set(split_words_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Warn user if there are split words\n",
    "if len(df_import_split_words) > 0:\n",
    "    print(\"WARNING: Split words found\")\n",
    "    print(df_import_split_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function doing all of that already in package `analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from analysis.tokens import find_important_split_words\n",
    "\n",
    "find_important_split_words(df_sentences, embedder.tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our application onboarding we want to issue warnings if this happens, and cue for a fine-tuning of the pre-trained transformer on the body of text so that the Transformer gets a better sence of those words..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will run the model to look in particular at the embeddings (without fine-tuning the language model) and look at calibration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Looking at Hugging Face Transformer embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "embeddings = embedder.encode(df_sentences, show_progress_bar=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.0 Checking embeddings have a \"sense\" of language used in FAQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite above warnings, let's verify the HF Transformer embeddings has a reasonable sense of similar sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "\n",
    "def most_similar(my_sentence, top_n=5):\n",
    "    emb = embedder.encode(my_sentence)\n",
    "    cosines = util.cos_sim(emb, embeddings)\n",
    "    similarities = {s: float(cosines[0][i]) for i, s in enumerate(df_sentences)}\n",
    "    ordered_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ordered_similarities[:top_n]\n",
    "\n",
    "\n",
    "most_similar(\"Can my dog transmit COVID-19 to me?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the transformer probably understands that the word `dog` (that does not appear in original FAQ) is close to the word `pet`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Visualize in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=(\"./embeddings\"))\n",
    "writer.add_embedding(embeddings, df_sentences, tag=\"My Embeddings\")\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# % load_ext tensorboard\n",
    "# % tensorboard  --logdir ./embeddings\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Visualize using UMAP with Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "import umap\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the dataset and a bit of our own out-of-domain sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset for UMAP\n",
    "test_embeddings = embeddings.copy()\n",
    "test_sentences = df_sentences.copy()\n",
    "# Add a column of zeros to represent the label\n",
    "test_embeddings = np.hstack((test_embeddings, np.zeros((test_embeddings.shape[0], 1), dtype=np.int32)))\n",
    "\n",
    "def add_new_sents(test_embeddings, test_sentences, new_sentences, label):\n",
    "    np_new_sents = np.array(new_sentences).reshape(-1, 1)\n",
    "    np_labels = np.full_like(np_new_sents, label, dtype=np.float64)\n",
    "    test_embeddings = np.vstack((test_embeddings, np.hstack((embedder.encode(new_sentences), np_labels))))\n",
    "    test_sentences = pd.concat([test_sentences, pd.Series(new_sentences)])\n",
    "    test_sentences.index = range(len(test_sentences))\n",
    "    return test_embeddings, test_sentences\n",
    "\n",
    "test_embeddings, test_sentences = add_new_sents(test_embeddings, test_sentences, \n",
    "                                                [\"I love playing soccer ?\", \"Do you eat cheese very often ?\", \"Can I play some chess with you?\", \"I liked a lot this movie we looked at yesterday\", \"Can I help you with something else?\", \"Good morning, my name is John\"], 1)\n",
    "\n",
    "test_embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train the model: this will fit a guassian with a center and covariance on the embeddings space. It allows us to get a pointer for the center of the distribution. Nicer for visualization..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.fit(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "location, _ = model.distribution.get_dist_params()\n",
    "test_embeddings = np.vstack((test_embeddings, np.hstack((location, 2)))) # Add center with label 2 to have another color...\n",
    "test_sentences = np.append(test_sentences, \"DISTRIBUTION CENTER\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reduce dimensionality with UMAP\n",
    "reducer = umap.UMAP(n_neighbors=24, metric='cosine', n_epochs=1000)\n",
    "X = test_embeddings[:, :-1]\n",
    "# scaled_embeddings = StandardScaler().fit_transform(X)\n",
    "scaled_embeddings = X\n",
    "umap_embedding = reducer.fit_transform(scaled_embeddings)\n",
    "umap_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot the UMAP projection\n",
    "sns_palette = sns.color_palette()\n",
    "color_ids = [int(label) for label in test_embeddings[:, -1]]\n",
    "my_palette = [sns_palette[i] for i in color_ids]\n",
    "plt.scatter(\n",
    "    umap_embedding[:, 0],\n",
    "    umap_embedding[:, 1],\n",
    "    c=my_palette)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the sentence embeddings', fontsize=24)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of domain sentences appear in red. The distribution center in green."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Interactive visualization with Bokeh\n",
    "\n",
    "Bokeh will allow us to see to which sentence each points corresponds. In below viz, mouse over some points to see which sentence it relates to. We can see that the OOD points are close to each other, but the OOD-point that corresponds to a sentence with `cheese` gets positionned close to an ID sentence about \"food and gatrointestinal and stomach illness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(umap_embedding, columns=('x', 'y'))\n",
    "embeddings_df['ood'] = [str(x) for x in test_embeddings[:, -1].astype(int)]\n",
    "embeddings_df['sentence'] = list(test_sentences)\n",
    "\n",
    "datasource = ColumnDataSource(embeddings_df)\n",
    "color_mapping = CategoricalColorMapper(factors=[\"0\",\"1\", \"2\"],\n",
    "                                       palette=(\"#0000ff\", \"#ff0000\", \"#00ff00\"))\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='UMAP projection of the dataset',\n",
    "    width=800,\n",
    "    height=600,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <span style='font-size: 10px'>@sentence</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=datasource,\n",
    "    color=dict(field='ood', transform=color_mapping),\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=4\n",
    ")\n",
    "show(plot_figure)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gaussian fit to the embeddings\n",
    "\n",
    "It is hard to show much about it. We already vizualized the center of the distribution that has been fit on the embeddings vector space. It will be nice to vizualize the `mahalanobis distance` between the sentence and various sentences. We will do that as part of the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Looking at mahalanobis distances and Calibrate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make up some sentences that are ID and OOD and add some random ones from our datasets. The calibrator will attempt to compute the best cutoff distance of \"mahalanobis\" distance for a sentence to be considered OOD: if a sentence is further away from the \"center\" of the Gaussian distribution it is considered OOD. Below:\n",
    "- \"score\" represents the raw mahalanobis distance between the center and the tested sentence. \n",
    "- \"delta\" represents the distance from the cutoff. A delta of zero means the sentence is on this frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pipelines.impl.anomaly_detection import OnInvalidSentence\n",
    "\n",
    "ood_sentences = [\n",
    "    \"My brother got a headache\",\n",
    "    \"How can I assist you?\",\n",
    "    \"How can I help you?\",\n",
    "    \"Welcome to our FDA support center\",\n",
    "    \"Goodbye, have a nice day\",\n",
    "    \"Anything else I can assist you with?\",\n",
    "    \"Sorry to hear that\",\n",
    "    \"I love eating pizza\",\n",
    "    \"I am not allowed to wear glasses\",\n",
    "    \"Welcome to our hotline\"\n",
    "]\n",
    "\n",
    "id_sentences = [\n",
    "    \"Do I need to wear a mask to protect myself?\",\n",
    "    \"Does hydroxychloroquine help to treat COVID-19?\",\n",
    "    \"Can cats transmit the illness to humans?\",\n",
    "    \"How much vaccines do I need to take?\",\n",
    "    \"I have food allergies, can I still take the vaccine?\",\n",
    "    \"As a smoker do I have more risks?\",\n",
    "    \"Are foreign foods dangerous?\",\n",
    "    \"After taking the vaccine will I be able to get children\"\n",
    "]\n",
    "\n",
    "model.recalibrate(id_sentences, ood_sentences, registry=datasets, on_invalid_sentence=OnInvalidSentence.WARN)\n",
    "model.calibrator.cutoff_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Calibration as a table\n",
    "\n",
    "In the on-boarding app we will allow the user to enter some sentences to calibrate himself. We will also provider a slider for the user to adjust: he will see in-live which sentences are becoming ID or OOD depending on it: this allows the user to adjust precision and recall to his liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calibration_sentences = [*id_sentences, *ood_sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "raw_id_scores = model.train_pipe.transform(id_sentences)\n",
    "raw_ood_scores = model.train_pipe.transform(ood_sentences)\n",
    "\n",
    "\n",
    "df_id = pd.DataFrame({\"score\": raw_id_scores, \"origin\": \"ID\"})\n",
    "df_ood = pd.DataFrame({\"score\": raw_ood_scores, \"origin\": \"OOD\"})\n",
    "df = pd.concat([df_id, df_ood])\n",
    "df[\"sentence\"] = [*id_sentences, *ood_sentences]\n",
    "\n",
    "# get the top 10 rows where abs(delta) is closest to 0\n",
    "df = df.sort_values(by=\"score\", ascending=False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pred, sent = model.predict_proba(calibration_sentences)\n",
    "pd.DataFrame({\"sentence\": sent, \"OOD prob\": pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.calibrator.r_id_, model.calibrator.r_ood_, model.calibrator.cutoff_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Calibration as an histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"darkgrid\")\n",
    "graph = sns.histplot(data=df, x=\"score\", hue=\"origin\", legend=True, stat=\"count\", palette=[\"red\", \"lightblue\"], alpha=0.5, kde=True)\n",
    "graph.axvline(model.calibrator.cutoff_, color=\"grey\", linestyle=\"--\", label=\"Cutoff\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Confusion matrix and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "original_cutoff = model.calibrator.cutoff_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from analysis.evaluate import evaluate_model\n",
    "# model.calibrator.cutoff, model.calibrator.adjusted_cutoff\n",
    "# In interface allow user to change the tradeoff\n",
    "adjustment = 0 # add positive amount for more recall, negative for more precision (less false positives)\n",
    "model.calibrator.cutoff_ = model.calibrator.cutoff_ + adjustment\n",
    "# You can adjust the cutoff to get a different tradeoff...\n",
    "scores = evaluate_model(model, id_sentences, ood_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "print(f\"F1-SCORE: {scores.f1}\")\n",
    "\n",
    "print(f\"False positives ({len(scores.fp_indices)}):\")\n",
    "for i in scores.fp_indices:\n",
    "    print(\"    \" + id_sentences[i])\n",
    "\n",
    "print(f\"False negatives ({len(scores.fn_indices)}):\")\n",
    "for i in scores.fn_indices:\n",
    "    print(\"    \" + ood_sentences[i])\n",
    "\n",
    "cm = confusion_matrix(scores.y_true, scores.y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LiMe requires a predict_one function to explain an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pipelines.impl.anomaly_detection import GaussianEmbeddingsAnomalyDetector\n",
    "from pipelines.filtering import FilteredSentence\n",
    "\n",
    "def predict_one(sentences):\n",
    "    # print(f\"Sents: {sentences[:5]}\")\n",
    "    results = []\n",
    "    for sentence in sentences:\n",
    "        scored_sentences = list(model.filter_sentences(sentence))\n",
    "        if len(scored_sentences) == 0:\n",
    "            results.append([0.0, 1.0])\n",
    "        if len(scored_sentences) > 1:\n",
    "            raise Exception(f\"More than one result in {scored_sentences}\")\n",
    "        if len(scored_sentences) == 1:\n",
    "            ood_score = scored_sentences[0].score / 100\n",
    "            id_score = 1 - ood_score\n",
    "            results.append([id_score, ood_score])\n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "\n",
    "predict_one([\"How much vaccines do I need to take?\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_one(sentences):\n",
    "    embeddings = model.embedder.transform(sentences)\n",
    "    raw_scores = model.distribution.transform(embeddings)\n",
    "    ood_probas = model.calibrator.predict_proba(raw_scores)\n",
    "    # Concatenate iid and ood as a 2D array\n",
    "    new_var = np.vstack([1 - ood_probas, ood_probas]).T\n",
    "    return new_var\n",
    "predict_one([\"How much vaccines do I need to take?\", \"What are you doing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=['no', 'yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def explain(sentence, num_features=4):\n",
    "    exp = explainer.explain_instance(\n",
    "        sentence,\n",
    "        predict_one,\n",
    "        num_features=num_features)\n",
    "    exp.show_in_notebook(text=sentence)\n",
    " \n",
    "explain(\"Do I need a car jacket and a safety belt?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(\"How can I help you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(\"Goodbye, see you later\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(\"Do I need to wear a mask to protect myself?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(\"Can my pet be infected also?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(\"A backwards poet writes inverse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(\"How much vaccine do I need to take?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "iid = datasets.load_items(model.datasets.validation_id)\n",
    "\n",
    "explain(iid[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(iid[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ood = datasets.load_items(model.datasets.validation_ood)\n",
    "explain(ood[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(ood[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(ood[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(\"Can I buy a movie?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "explain(\"Welcome to our FDA support center\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a lot of ID sentences are \"How much do I need to take?\" \"How many\" or \"Can I\" some sentences tend to be marked ID but they are really OOD. Removing stop words seems to solve the issue but this needs confirmation, and might force us to retrain the HuggingFace transformer in all situations: because our language is not exactly the same with and without stop words..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utterances-filter-znCQzly0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b308ff8890058ccc2706e4ff6700c9191719e33c8cceaa2458186882de2437df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
